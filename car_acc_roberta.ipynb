{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-774c8c20de1b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdata_row\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitertuples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m     \u001b[0mDATA_LIST\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_row\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_categorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_row\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m \u001b[0mDATA_LIST\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATA_LIST\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[0mdf_pre\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_excel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data_car/pesudodata.xlsx'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence"
     ]
    }
   ],
   "source": [
    "#! -*- coding:utf-8 -*-\n",
    "import re, os, json, codecs, gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from random import choice\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import KFold\n",
    "from keras_bert import load_trained_model_from_checkpoint, Tokenizer\n",
    "\n",
    "from keras.layers import *\n",
    "from keras.callbacks import *\n",
    "from keras.models import Model\n",
    "import keras.backend as K\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "#train_label = pd.read_csv('data_car/test.csv')\n",
    "#train_df = pd.merge(train_df, train_label, on='id')\n",
    "train_df = pd.read_csv('data_car/train.csv')\n",
    "test_df = pd.read_csv('data_car/test.csv')\n",
    "#test_lines = codecs.open('data_car/test.csv').readlines()[1:]\n",
    "train_df['tb'] = train_df['title'].replace(' ', '') + train_df['content'].replace(' ', '')\n",
    "test_df['tb'] = test_df['title'].replace(' ', '') + test_df['content'].replace(' ', '')\n",
    "train_df['tb'] =train_df['tb'].astype('str')\n",
    "test_df['tb'] = test_df['tb'].astype('str')\n",
    "#! -*- coding:utf-8 -*-\n",
    "import re, os, json, codecs, gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from random import choice\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import KFold\n",
    "from keras_bert import load_trained_model_from_checkpoint, Tokenizer\n",
    "import tokenization\n",
    "from keras.layers import *\n",
    "from keras.callbacks import *\n",
    "from keras.models import Model\n",
    "import keras.backend as K\n",
    "from keras.optimizers import Adam\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\" # 按照PCI_BUS_ID顺序从0开始排列GPU设备 \n",
    "\n",
    "maxlen = 512\n",
    "config_path = '/home/jovyan/work/data/chinese_L-12_H-768_A-12/bert_config.json'\n",
    "# checkpoint_path = '/export/home/liuyuzhong/kaggle/bert/chinese_L-12_H-768_A-12/bert_model.ckpt'\n",
    "checkpoint_path = '/home/jovyan/work/data/chinese_L-12_H-768_A-12/bert_model.ckpt'\n",
    "dict_path = '/home/jovyan/work/data/chinese_L-12_H-768_A-12/vocab.txt'\n",
    "\n",
    "token_dict = {}\n",
    "with codecs.open(dict_path, 'r', 'utf8') as reader:\n",
    "    for line in reader:\n",
    "        token = line.strip()\n",
    "        token_dict[token] = len(token_dict)\n",
    "\n",
    "class OurTokenizer(Tokenizer):\n",
    "    def _tokenize(self, text):\n",
    "        R = []\n",
    "        for c in text:\n",
    "            if c in self._token_dict:\n",
    "                R.append(c)\n",
    "            elif self._is_space(c):\n",
    "                R.append('[unused1]') # space类用未经训练的[unused1]表示\n",
    "            else:\n",
    "                R.append('[UNK]') # 剩余的字符是[UNK]\n",
    "        return R\n",
    "\n",
    "tokenizer = OurTokenizer(token_dict)\n",
    "tokenizer1 = tokenization.FullTokenizer(vocab_file=dict_path, do_lower_case=True)\n",
    "def seq_padding(X, padding=0):\n",
    "    L = [len(x) for x in X]\n",
    "    ML = max(L)\n",
    "    return np.array([\n",
    "        np.concatenate([x, [padding] * (ML - len(x))]) if len(x) < ML else x for x in X\n",
    "    ])\n",
    "\n",
    "def dategenenrator(data, batch_size=1, shuffle=True):\n",
    "    X1, X2, Y = [], [], []\n",
    "    idxs = list(range(len(data)))\n",
    "    if shuffle:\n",
    "        np.random.shuffle(idxs)\n",
    "    for i in idxs:\n",
    "        d = data[i]\n",
    "        text = d[0][:maxlen-2]\n",
    "        x1,x2=tokenizer.encode(first=text)\n",
    "        y = d[1]\n",
    "        X1.append(x1)\n",
    "        X2.append(x2)\n",
    "        Y.append([y])\n",
    "    X1 = seq_padding(X1)\n",
    "    X2 = seq_padding(X2)\n",
    "    Y = seq_padding(Y)\n",
    "    Y = np.array([np.argmax(x) for x in Y])\n",
    "    return [X1,X2],Y\n",
    "\n",
    "def convert_lines(example, max_seq_length,tokenizer):\n",
    "    max_seq_length -=2\n",
    "    all_tokens = []\n",
    "    all_mask=[]\n",
    "    longer = 0\n",
    "    for i in range(example.shape[0]):\n",
    "      tokens_a = tokenizer.tokenize(example[i])\n",
    "      if len(tokens_a)>max_seq_length:\n",
    "        #tokens_a = tokens_a[:int(max_seq_length/2)] +tokens_a[-int(max_seq_length/2):]\n",
    "        \n",
    "        tokens_a = tokens_a[:max_seq_length]        \n",
    "        longer += 1\n",
    "        mask=[1]*(max_seq_length+2)\n",
    "      else:\n",
    "        mask=[1]*(len(tokens_a)+2)+[0]*(max_seq_length-len(tokens_a))\n",
    "      one_token = tokenizer.convert_tokens_to_ids([\"[CLS]\"]+tokens_a+[\"[SEP]\"])+[0] * (max_seq_length - len(tokens_a))\n",
    "      all_tokens.append(one_token)\n",
    "      all_mask.append(mask)\n",
    "    print(longer)\n",
    "    return np.array(all_tokens),np.array(all_mask)\n",
    "\n",
    "class data_generator:\n",
    "    def __init__(self, data, batch_size=1, shuffle=True):\n",
    "        self.data = data\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.steps = len(self.data) // self.batch_size\n",
    "        if len(self.data) % self.batch_size != 0:\n",
    "            self.steps += 1\n",
    "    def __len__(self):\n",
    "        return self.steps\n",
    "    def __iter__(self):\n",
    "        while True:\n",
    "            idxs = list(range(len(self.data)))\n",
    "            \n",
    "            if self.shuffle:\n",
    "                np.random.shuffle(idxs)\n",
    "            \n",
    "            X1, X2, Y = [], [], []\n",
    "            for i in idxs:\n",
    "                d = self.data[i]\n",
    "                text = d[0][:maxlen-2]\n",
    "                x1, x2 = tokenizer.encode(first=text)\n",
    "                y = d[1]\n",
    "                X1.append(x1)\n",
    "                X2.append(x2)\n",
    "                Y.append([y])\n",
    "                if len(X1) == self.batch_size or i == idxs[-1]:\n",
    "                    X1 = seq_padding(X1)\n",
    "                    X2 = seq_padding(X2)\n",
    "                    Y = seq_padding(Y)\n",
    "                    yield [X1, X2], Y[:, 0, :]\n",
    "                    [X1, X2, Y] = [], [], []\n",
    "\n",
    "from keras.metrics import top_k_categorical_accuracy\n",
    "def acc_top2(y_true, y_pred):\n",
    "    \n",
    "    return top_k_categorical_accuracy(y_true, y_pred, k=2)\n",
    "                    \n",
    "def build_bert(nclass):\n",
    "    bert_model = load_trained_model_from_checkpoint(config_path, checkpoint_path, seq_len=None)\n",
    "\n",
    "    for l in bert_model.layers:\n",
    "        l.trainable = True\n",
    "\n",
    "    x1_in = Input(shape=(None,))\n",
    "    x2_in = Input(shape=(None,))\n",
    "\n",
    "    x = bert_model([x1_in, x2_in])\n",
    "    x = Lambda(lambda x: x[:, 0])(x)\n",
    "    p = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "    model = Model([x1_in, x2_in], p)\n",
    "    model = multi_gpu_model(model,gpus=[5,6])\n",
    "    model.compile(loss='binary_crossentropy', \n",
    "                  optimizer=Adam(1e-5),\n",
    "                  metrics=['accuracy', acc_top2])\n",
    "    print(model.summary())\n",
    "    return model\n",
    "from keras.utils import to_categorical\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\" \n",
    "DATA_LIST = []\n",
    "for data_row in train_df.iloc[:].itertuples():\n",
    "    DATA_LIST.append((data_row.tb, to_categorical(data_row.flag, 2)))\n",
    "DATA_LIST = np.array(DATA_LIST)\n",
    "\n",
    "df_pre = pd.read_excel('data_car/pesudodata.xlsx')\n",
    "df_pre = df_pre[pd.isna(df_pre.pre)==False]\n",
    "df_pre['flag'] = df_pre['pre']\n",
    "df_pre['tb'] = df_pre['title'].replace(' ', '') + df_pre['content'].replace(' ', '')\n",
    "df_pre['tb'] = df_pre['tb'].astype('str')\n",
    "DATA_LIST_PRE = []\n",
    "for data_row in df_pre.iloc[:].itertuples():\n",
    "    DATA_LIST_PRE.append((data_row.tb, to_categorical(data_row.flag, 2)))\n",
    "DATA_LIST_PRE = np.array(DATA_LIST_PRE)\n",
    "\n",
    "DATA_LIST_TEST = []\n",
    "for data_row in test_df.iloc[:].itertuples():\n",
    "    DATA_LIST_TEST.append((data_row.tb, to_categorical(0, 2)))\n",
    "DATA_LIST_TEST = np.array(DATA_LIST_TEST)\n",
    "def run_cv(nfold, data, data_label, data_test):\n",
    "        \n",
    "    kf = KFold(n_splits=nfold, shuffle=True, random_state=520).split(data)\n",
    "    train_model_pred = np.zeros((len(data), 1))\n",
    "    test_model_pred = np.zeros((len(data_test), 1))\n",
    "    #oof_train = np.zeros((len(train),1))\n",
    "    #oof_test = np.zeros((len(test_df),1))\n",
    "    for i, (train_fold, test_fold) in enumerate(kf):\n",
    "        X_train, X_valid, = data[train_fold, :], data[test_fold, :]\n",
    "        #X_train = np.concatenate((X_train, DATA_LIST_PRE))\n",
    "        model = build_bert(2)\n",
    "        early_stopping = EarlyStopping(monitor='val_acc', patience=3)\n",
    "        plateau = ReduceLROnPlateau(monitor=\"val_acc\", verbose=1, mode='max', factor=0.5, patience=2)\n",
    "        checkpoint = ModelCheckpoint('baseline2/base_roberta/' + str(i) + '.hdf5', monitor='val_acc', \n",
    "                                         verbose=2, save_best_only=True, mode='max',save_weights_only=True)\n",
    "        \n",
    "        train_D,train_Y= dategenenrator(X_train, shuffle=True)\n",
    "        valid_D,valid_Y = dategenenrator(X_valid, shuffle=True)\n",
    "        test_D,test_Y = dategenenrator(data_test, shuffle=False)\n",
    "        \n",
    "        \n",
    "        model.fit(train_D,train_Y,\\\n",
    "               validation_data = (valid_D,valid_Y),\n",
    "               callbacks=[early_stopping, plateau, checkpoint],batch_size=1,epochs=5,shuffle=True)\n",
    "        \n",
    "        '''model.fit_generator(\n",
    "            train_D.__iter__(),\n",
    "            steps_per_epoch=len(train_D),\n",
    "            epochs=5,\n",
    "            validation_data=valid_D.__iter__(),\n",
    "            validation_steps=len(valid_D),\n",
    "            callbacks=[early_stopping, plateau, checkpoint],\n",
    "        )'''\n",
    "        \n",
    "        #model.load_weights('bert_dump_base/' + str(i) + '.hdf5')\n",
    "        \n",
    "        # return model\n",
    "        train_model_pred[test_fold, :] =  model.predict(valid_D,verbose=1)\n",
    "        test_pred_temp =  model.predict(test_D,verbose=1)       \n",
    "        test_model_pred += test_pred_temp\n",
    "        #oof_train[idx_test] = test_pred_temp\n",
    "        x_pre = model.predict(test_D)\n",
    "        test_df['label'] = x_pre\n",
    "        test_df.to_csv('baseline2/base_roberta/1029'+str(i)+'.csv',index=False) \n",
    "        #oof_test += x_pre/skf.n_splits\n",
    "        del model; gc.collect()\n",
    "        K.clear_session()\n",
    "        \n",
    "        # break\n",
    "        \n",
    "    return train_model_pred, test_model_pred\n",
    "train_model_pred, test_model_pred = run_cv(10, DATA_LIST, None, DATA_LIST_TEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_model_pred' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-46e4d0e47f66>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mlistvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mprevalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredictvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_model_pred\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprevalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'baseline2/base_roberta/baseline3_1029.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_model_pred' is not defined"
     ]
    }
   ],
   "source": [
    "def predictvalue(oof_test):\n",
    "    listvalue = []\n",
    "    for i in range(len(oof_test)):\n",
    "        if oof_test[i] > 0.5:\n",
    "            listvalue.append(1)\n",
    "        else:\n",
    "            listvalue.append(0)\n",
    "    return listvalue\n",
    "\n",
    "prevalue = predictvalue(test_model_pred/10)\n",
    "test_df['label'] = prevalue\n",
    "test_df[['id', 'label']].to_csv('baseline2/base_roberta/baseline3_1029.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "testpre = pd.DataFrame({'id':test_df['id'],'label':test_df['labelpre']})\n",
    "testpre.to_csv('testpredictcar_pesudo_1024.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'threshold': 0.5033168196678162, 'f1': 0.8451261751608115}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:7: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "df['pre']=oof_train\n",
    "df.to_csv('trainpredictcar_pesudo_1024.csv',index=False)\n",
    "from sklearn.metrics import roc_curve, precision_recall_curve\n",
    "def threshold_search(y_true, y_proba, plot=False):\n",
    "    precision, recall, thresholds = precision_recall_curve(y_true, y_proba)\n",
    "    thresholds = np.append(thresholds, 1.001) \n",
    "    F = 2 / (1/precision + 1/recall)\n",
    "    best_score = np.max(F)\n",
    "    best_th = thresholds[np.argmax(F)]\n",
    "    if plot:\n",
    "        plt.plot(thresholds, F, '-b')\n",
    "        plt.plot([best_th], [best_score], '*r')\n",
    "        plt.show()\n",
    "    search_result = {'threshold': best_th , 'f1': best_score}\n",
    "    return search_result \n",
    "\n",
    "result = threshold_search(df.flag.values,oof_train)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
